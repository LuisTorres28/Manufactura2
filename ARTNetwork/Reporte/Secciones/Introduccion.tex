Una red neuronal esta compuesta por varios perceptrones y su salida sigue las mismas reglas que el
perceptrón, es decir, se realiza la suma ponderada de las entradas y después esta se hace pasar por una
función de activación. Esta operación se realiza de manera recursiva de manera que, la entrada de la capa
subsecuente corresponde a la salida de la capa actual. De esta manera, mediante la interconexión de
varios perceptrones es posible generar funciones no lineales.\\
La manera en que aprenden las redes neuronales es mediante la minimización de una función de costo, como
puede ser el error cuadrático medio (MSE), ReLu, entre otras. Una vez que se tiene la función de costo,
se utiliza la técnica del descenso del gradiente para minimizar esta función y mediante esta técnica la
red neuronal ``aprende''. Como técnica utilizada para obtener el gradiente se utiliza el algoritmo de
\textbf{backpropagation}.